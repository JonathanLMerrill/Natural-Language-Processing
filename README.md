# Natural-Language-Processing
The collection of Natural language processing projects I worked on during my senior year at BYU

### Bible Text Analysis
I leveraged NLTK to tokenize a corpus of Bible verses and extract valuable insights about word frequency and co-occurrence. Specifically, I identified the verses in which a single word appears over 10 times and recorded the word and its count in a CSV file, named 'NLP_Task2.csv'. Moreover, I identified the top two most frequent words in each verse and pinpointed the instances where two different words co-occur at least 8 times. These results were stored in a separate CSV file named 'NLP_Task2b.csv'. Remarkably, I discovered 52 verses that met this criteria, despite the prevalence of small, common words like 'the', 'of', and 'and'. One intriguing example is 2 Corinthians 11:26, which features the word 'in' 11 times and 'perils' 8 times. Through this exercise, I gained a deeper understanding of tokenization and NLTK's powerful capabilities.
### Movie Review Sentiment Analysis
I conducted a sentiment analysis of movie reviews using the Vader tool, specifically focusing on a movie that received polarized reviews from critics and the general audience. To test the accuracy of the sentiment analysis, I deliberately included positive and negative reviews that contained mixed sentiments.

For the positive reviews, I anticipated that they would generally receive positive scores due to the use of positive language such as "beautiful," "thrilling," "engaging," "touching," "good," and "excellent." However, I also included a positive review that contained some negative elements, such as "betrayal," "Anthropophobia," "bullying," and "exploiters," to test the efficacy of the sentiment analysis in identifying mixed sentiments. While other positive reviews also contained negative words, I believe that the overall positivity of the reviews will outweigh any negativity.

For the negative reviews, I anticipated that they would receive profoundly negative scores due to the use of negative words such as "boring," "lacks drama," "bland," "lifeless," "predictable," and "confusing." Similarly to the positive reviews, I included a negative review that contained some positive elements, such as "gorgeous visuals" and a "committed performance" by the lead actress, to test the ability of the sentiment analysis to identify mixed sentiments. While the first half of the review may initially appear positive, I predict that the sentiment analysis will ultimately identify the overall negative sentiment towards the movie.

Overall, this sentiment analysis provides insights into the effectiveness of the Vader tool in identifying mixed sentiments in movie reviews.
### N Gram Bible Generator
I conducted an experiment with NLTK by varying the number of n-grams used on a Bible corpus to generate new Bible verses. I discovered that as I increased the number of grams, the generated verses became more coherent and less fragmented. However, due to the long computation time and questionable results of 9-grams, I found that 7-grams produced a more accurate outcome without overfitting the data. It was intriguing to note that as I increased the number of grams, the generated verses increasingly resembled actual Bible verses. Notably, using 7-grams yielded a verse that corresponded to Luke 10:38, 8-grams produced a verse that aligned with Jeremiah 22:3, and 9-grams generated a verse similar to 1 Chronicles 27:26. Overall, this experiment provided valuable insights into the concept of n-grams and their impact on language generation.
### NLTK Name Entity Recognition
 I used Named Entity Recognition with NLTK's built in function. I followed the instructions and got similar results as they posted in the NLTK book. The program is supposed to identify 9 different types of Named Entities: Organization, person, location, date, time, money, percent, facility, and geo-political entity.
### Naive Bayes Classifier 
I utilized NLTK to implement a Naive Bayes classifier for predicting the origin of Bible verses. Initially, I tagged the verses as either from the Old Testament or the New Testament and created features based on the most common words in each verse. The first iteration of the classifier yielded an accuracy of approximately 88%, which increased to about 93% after omitting the conversion of words to lowercase. Interestingly, the presence of "LORD" made it 49.8 times more likely for a verse to be from the Old Testament, while "Lord" made it 32.9 times more likely for a verse to be from the New Testament.

Subsequently, I expanded the analysis to include the Book of Mormon, which yielded an accuracy of approximately 87%. Furthermore, I analyzed the four Gospels (Matthew, Mark, Luke, and John), and the classifier achieved an accuracy of around 51%, which is reasonable considering the writers wrote about the same events. Additional features such as part-of-speech tagging could be incorporated to improve the accuracy of the classifier. Overall, this project helped me gain insights into the use of Naive Bayes classification in text analysis.
### Part of Speech tagging
I employed part of speech tagging techniques using NLTK to analyze the Declaration of Independence, and was able to identify and differentiate between common and proper nouns, adjectives, adverbs, and even numbers. I discovered that the text's capitalized words created issues with proper noun identification, but these were resolved by converting the entire text to lowercase. The NLTK program demonstrated impressive accuracy in detecting the correct part of speech, verb tense, and voice, even in instances where it was challenging for me, a native English speaker, to do so. The Declaration of Independence, with its unique combination of collective first-person voice and imperative statements, was an especially intriguing text to examine in this way. Overall, this exercise with part of speech tagging deepened my appreciation for the power of natural language processing techniques in analyzing complex written works.
### Pronunciation Analysis
In this project, I utilized the natural language toolkit's (nltk) CMU pronouncing dictionary for US English to analyze patterns and statistics in word pronunciation. Out of the 133,737 words in the corpus, I discovered that 9,241 of them had more than one pronunciation, with only two words having more than four different pronunciations - February and February's. To establish rules for determining when words should have multiple pronunciations, I examined the commonalities between words with two pronunciations and identified the most frequently changing phonemes. My analysis revealed that the AH0 phoneme (the unstressed "ah" sound) was the most commonly switched pronunciation, appearing as an alternate pronunciation 2,361 times throughout the corpus. Interestingly, the UW phoneme had the highest likelihood of having multiple pronunciations, having occurred only once but with an alternate pronunciation in that instance.
### Sentiment Analysis
I did a sentiment analysis of different words specifically focusing on religions and sports. 
### Spacy Dependency Parsing
I used Spacy for Dependency Parsing. The interface allowed for seamless utilization and included a visualization option for parsing trees which gave a detailed representation of each word's placement within the sentence. To further acquaint myself with the software, I input a corpus of Bible verses to extract the head of each word within the sentence. Despite the complexity of some of the longer verses that contained intricate noun and verb phrases, Spacy performed exceptionally well. However, there were some challenges in parsing sentences that contained multiple conjunctions, appositive phrases, participial phrases, and absolute phrases. 
